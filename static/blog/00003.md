# 笔记 

## 介绍
----

### 本文针对什么领域

自然场景文本阅读，识别自然背景（包括但不限于纸质文档）下的文字

### 目前自然场景文本有哪些问题未解决

由于字有不同的外观，扭曲程度，低分辨率，模糊，背景干扰，无约束环境下的文本识别是个大问题。

### 其他论文/方法有哪些不足

网络深参数量过大，难以训练，对于各种噪声干扰下的识别的鲁棒性不够。

### 本文主要贡献

提出了一个简单而高效的端到端的全卷积网络，该模型在四种标准的基准测试上达到了state-of-art，这种网络相比于基于RNN的文字识别网络的计算量要更少，该模型替换掉RNN/LSTM/BLSTM部分的卷积层的参数量减少了三个数量级，运行时间减少了36倍

相关方法结构
------------

-   HighWay Network：加入了类似于LSTM的门机制

```latex
x_{l} = T_{l}\left( x_{l - 1} \right)*M\left( x_{l - 1} \right) + x_{l - 1}*C(x_{l - 1})
```

```latex
\Rightarrow x_{l} = T_{l}\left( x_{l - 1} \right)*M\left( x_{l - 1} \right) + x_{l - 1}*(1 - M(x_{l - 1}))
```

-   ResNet: 去除门函数的作用直接获得残差函数

```latex
x_{l} = T_{l}\left( x_{l - 1} \right) + x_{l - 1}
```

-   DenseNet: 取消ResNet中直接相加的操作，改为所有特征进行连接

```latex
x_{l} = T_{l}\left( \lbrack{x_{0},x_{1},\ldots,x}_{l - 1}\rbrack \right)
```

本文提出的模型
--------------

### 方法结构

![结构图](/static/blog/media/6d062da278bbd370a22217e68a12813d.png)

>   本模型的总体结构分成了三个部分：

#### 注意力特征编码器（Attention feature encoder）

由于复杂的自然场景文字图中有较多较为复杂的干扰因素，如：阴影，无关符号，背景纹理等。许多的识字模型的效果差通常就是因为过于关注这类无意义的干扰导致无法集中进行文字特征提取任务。因此本文借鉴了残差注意力网络的部分结构用于在本模型中加入注意力机制。

那么，*这个残差注意力的结构究竟是什么样子的？*

这一结构取自于另一篇论文残差注意力网络，从两者的结构图中可以看出两者的结构相仿，所以我们首先通过另一篇论文介绍这一结构，再从这篇论文中讨论新的结构是否做出了什么创新和改变。

首先看一下原先的结构，单一的一个注意力模块中分出了两条分支，主干分支和掩膜分支

令主干分支的输出为$$T_x$$，掩膜分支使用Bottom-Up,Top-Down的方式实现一个编码解码器（Encoder-Decoder）来学习得出一个映射$$M_x$$作为$$T_x$$的软化权重输出特征，这样的一种结构模拟了快速的前向传播与反向传播使用注意力的过程，这一分支输出的掩膜图被用于作为主干分支中神经元的输出控制门（电路）这一控制门决定了那些特征将会被采用，哪些特征则是会被忽略。注意力模块的输出$$H_x$$用公式表达为：

```latex
H_{i,c}\left( x \right) = M_{i,c}\left( x \right)*T_{i,c}(x)
```

其中$$i$$为空间位置的遍历索引，$$c$$为通道遍历的索引。从这个公式中我们可以看出$$M(x)$$与$$T(x)$$的输出特征的尺寸是相同的。

然而这一种单纯的注意力结构反而会导致模型的效果变差，分析后是由以下几个原因造成的：

1.  重复的将取值为从0\~1的掩膜与特征图相乘会导致深层次的特征值被降低

2.  软化的掩膜反而会潜在的破坏主干支路上的优良的特征

因此残差注意力的作者给出了一种新的残差注意力的学习机制，将软化掩膜单元作为恒等映射，这样能够保证所得到的效果不会比没有注意力机制的差。我们把注意力模块的输出H表达修改为：

```latex
H_{i,c}(x)\  = \ (1\  + \ M_{i,c}(x))\ *\ F_{i,c}(x)
```

从公式中可以看出，这一种学习方式与之前的区别，前者是通过将注意力不集中的位置进行弱化，而后者则是通过将注意力集中的位置进行强化，即使$$M$$为0，输出的也是原本的特征$$F$$。通过这一种方式，解决了深度特征值降低和破坏好的特征值的问题。

那么回到本文作者的残差注意力机制，通过论文的阅读我们发现做做并未对这个残差注意力机制做出任何改变，公式也是原论文的简化版。

![](/static/blog/media/99305019a7dee50b7ce54576a7fcb1fa.png)

![](/static/blog/media/4d260e7766537f04097f2858a20dd5b9.png)

![](/static/blog/media/c170a73d5acd8d7db238d32616b0e9f6.png)

#### 卷积序贯建模（Convolutional sequence modeling）

这个是这篇论文所提出的主要创新，也是论文名字中使用全卷积网络进行文字序列的识别的由来。

近两年来较为流行的序列到序列的文字识别，语音识别，语言模型，机器翻译的网络的主要结构都是基于RNN的。这种循环的结构因为依赖于结构先前的输出，所以有一个很大的缺点就是不能够执行并行的操作，另外这种结构很容易会导致梯度消失与梯度爆炸使得网络训练较为困难。本文作者认为无论是RNN还是双向RNN的结构与卷积神经网络中的上下文敏感的感受野有着相似的效果，所以本文作者尝试着去使用卷积网络来代替循环网络来进行自然场景文本识别，用堆叠的卷积神经网络对得到的特征序列的元素以及其上下文信息进行建模。

*那么这样的方法如何去实现呢？*

![](/static/blog/media/ab9e733b3b2ffbc570687ea9be1ca43b.png)

**首先是解决如何处理特征的输入的问题**：由于将RNN类的层替换为了卷积层，特征的输入也需要进行相应的变化才能正常运行，故除了沿用先前CRNN模型的Map-to-sequence结构外，本文还加了一个Sequence-to-map的还原结构,将上一个结构得到的特征序列拼接成二维的特征图便于输入CNN中。*那为什么不直接将上一个编码器模块输出得到的三位特征图直接作为这一卷积层模块的输入呢？*作者的目的是想要证明他的卷积层模块同样具有处理序列的数据并且能够捕捉到序列的上下文信息，故保留这一结构用于和其他识别网络中的RNN结构进行实验结果对比。通过这一结构的转换特征序列被转换为了一个2维的特征图，显然该特征图中的每一列都代表了对应了原文字图像中一个上下顶格，左右长度固定的图像区域中提取得出的特征。

**然后是卷积的过程细节**：由于上一步提到了，输入特征图中自左向右的每一列其实都代表了原图中的一个图像区域。所以我们通过卷积操作进行特征提取。另外在本文中作者通过增加卷积层来增大感受野，*这是怎么一回事？*

```latex
Output = \frac{\text{Input} - k + 2p}{s} + 1
```

举个例子，假设输入为17\*17的2维矩阵经过四层使用卷积核大小为5，填充边维0，步长为1的卷积层后得到的输出为1，即输出的1\*1的特征图能够获取原图中17个输入的特征。然而在实际的处理过程中为保证输入的序列的宽度与输出的序列的宽度能够保持一致，作者在卷积操作之前进行了边缘填充0的操作。由于卷积后得到的特征图是3维的，即使切片后依然是2位的矩阵，而不是1维的向量，作者又增加了一步展开的步骤，将2维矩阵拉伸成为1维向量。至此，我们最终获得了$$w$$个向量作为CTC的输入。

![](/static/blog/media/f17391a9240e4948400ac4f0bc258da2.png)

### 方法创新

卷积序贯模型的提出的使得端到端的文字识别不要再借助RNN类的模型的使用，从而大幅的减少了模型参数量和运算时间。

本文提出的方法的效果如何
------------------------

### 数据集

本文所使用的训练数据集只有Synthetic数据集，该数据集包含了8百万张图片。但是使用的测试数据集较为繁多包括了Street
View Text，IIIT5K，ICDAR2003和ICDAR2013.将这四种数据集作为标准的基准测试集。

### 实验效果图

![](/static/blog/media/e564599afc69f8f4544b5f125ca6646d.png)

该图为在不同标准的自然场景下的文本数据集中的文本识别结果。A中为无关符号干扰下的识别情况，B中为阴影干扰，C中为背景纹理干扰，D为普通样本，E中的样本为错误分类的结果图的展示。

### 实验数据结果

#### 准确率比较

![](/static/blog/media/8533a9392277d899abf123075ba201cf.png)

这是一个基于四个标准的基准测试集进行的无字典情况下的场景文字识别正确率的比较实验。四个被比较的方法的唯一区别是模型在提取完的特征图前是否加入注意力机制的不同

和提取特征图后所使用的序列特征的特征提取器的不同。从实验数据结果中我们能够看到的是两种带有残差注意力模块网络都比原先的结构的正确率表现要比先前的高，证明了残差注意力结构是有效的。另外虽然使用了卷积网络的模型看起来似乎是没有BLSTM的好，但是在下述实验中我们能够看得出这一种结构在运行时间的表现上是非常的优秀的。

#### 运行时间比较

![](/static/blog/media/8ba5711308f714333d5ce4fb179f77da.png)
