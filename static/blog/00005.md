笔记
====

总览
----

### 论文名称

Shape Robust Text Detection with Progressive Scale Expansion Network

### 论文出处

CVPR2019

### 统计数据

|                                                                                                     | 单词数 | 配图数 | 表格数 | 公式数 | 引文  |
|-----------------------------------------------------------------------------------------------------|--------|--------|--------|--------|-------|
| 0. Abstract                                                                                         | 237    | 1      | 0      |        |       |
| 1. Introduction                                                                                     | 523    | 1      | 0      |        |       |
| 2. Related Work                                                                                     | 369    | 0      | 0      |        |       |
| 3.  Method                                                                                          | 1502   | 3      | 0      |        |       |
| 4. Experiment                                                                                       | 2381   | 2      | 6      |        |       |
| 5. Conclusion and Future Work                                                                       | 142    | 0      | 0      |        |       |
| 6. Acknowlegment                                                                                    | 54     | 0      | 0      |        |       |
| 7. Supplementary Materials for Shape RobustText Dectection with Progressive Scale Expansion Network | 575    | 6      | 1      |        |       |
| 总计                                                                                                | 5783   | 13     | 7      | 7      | 31/43 |

导读
----

### 本文针对什么领域

自然场景文本行检测

### 目前该领域有哪些问题未解决

有两大具有挑战性的问题：

1.  现有的state-of-art 的算法依赖四边形的Bounding
    Box导致对任意形状（弯曲，倾斜）的文本行定位会不准确

2.  距离过于接近的两个以上的文本行实例容易被检测为一个文本行导致失败

### 本文主要贡献

本文提出了一种递进的尺度扩展网络(Progressive Scale Expansion Network,
PSENet)，结合了**FPN(Feature Proposal Network,
FPN)**所提出的**Top-Down**结构的模型与本文独创的**Progressive Scale Expansion
算法**，使得它可以精确地检测任意形状的文本实例。

### 实验对比情况

在CTW1500, Total-Text, ICDAR 2015 和 ICDAR 2017 MLT上进行的验证实验

在含有较多弯曲文本的数据集CTW1500上的F-measure达到了74.3% 和FPS达到了27

本模型能够达到最高的F-measure值为82.2%，比目前的 state-of-art 高出 6.6%

解读
----

### 介绍

由于现有的大部分检测方法使用了基于bounding box
regression的定位方法，使得定位结果通常是四边形且方向固定的所以对特殊形状的文字效果非常差。另外对于靠的比较近的文本行进行分割是非常难的。为了解决以上两个问题，作者提出了新的基于**kernel**的一种新框架，取名为递进的尺度扩展网络(Progressive
Scale Expansion Network, PSENet)，这种模型有两点好处：

1.  它是一种像素级别的分割对于任意形状的文本行理论上都能精确定位

2.  模型使用作者提出的递进尺度扩展算法，使用这种算法能够使相邻的文本行被区分开

这种方法说的稍微具体一点就是对每一个文本行都分配多个称为kernel的分割区域，这些每一个实例的kernel的形状相似，但是大小不一样，但是得到的检测结果只需要也只能有一个，所以本模型通过一种基于广度优先搜索（BFS）的递进尺度扩展算法来确定一个结果。这种算法概括地来说分为三步：

1.  从最小尺度的kernel开始（在这一步中所有的文本行都能分开）

2.  通过将较大的kernel中所包含的像素逐渐加入到现在的区域中

3.  当最大的kernel的区域都被探索完毕时终止算法

这一个算法的设计思路有三点：

1.  最小尺度的kernel中各个文本行的区域离得远，容易被分开

2.  但是最小尺度的kernel无法覆盖整个文本行区域，所以要加入大kernel（如下图crnn使用固定的kernel导致文字无法识别）

3.  这种递进尺度扩展方法对于扩展kernel来说简单有效，保证精度

![](/static/blog/media/4253689604f977439f9dce060277b5ce.png)

### 方法结构

#### 总体结构

![](/static/blog/media/58f9ef62a43bcd2b649599f47c5a08ef.png)

Figure
3.是本文提出的网络框架结构的流程图，首先是虚线左侧的结构，这是一个带有跳跃连接的Top-Down结构，这个结构来自于FPN,
并且使用ResNet作为backbone，关于FPN的跳跃结构我们从原文中给出一张示意图。

![](/static/blog/media/0278459541581ba7eb25d5bcc7ffaa04.png)

#### 网络部分结构

通过上述结构能够得到一个特征金字塔， 其中包含了（*P5, P4, P3,
P2*）四个256维通道的特征图，为了能够将从低等到高等的语义特征信息合并，用一种函数C()来合并四个特征图并获得一个1024维通道的特征图F

函数C的表达为：

![](/static/blog/media/574c7b2fbbb47d570d2a332c68bd99d2.png)

其中 \|\| 代表将四个不同大小的特征图分别上采样后得到特征图进行拼接。

随后，得到的特征图F会在经过一次3x3的卷积层后被降维到256维通道数，最后再经过一个*n*维的1x1的卷积层后给出一个*n*维通道数的分类结果*S1，S2,
…, Sn*。

最后是虚线框内的递进尺度扩展算法，这一种算法的中心思想是广度优先搜索（BFS）。使

用原文中的给出的例子

#### 算法部分结构

![](/static/blog/media/44b44bba0d5cfe0d377801c75ffb9a3d.png)

上图是递进式尺度扩展算法的流程图，CC是连通域搜索算法，EX是尺度扩展算法，图中可见，有三个分割结果图*S*
= { *S1, S2, S3* }, 最小的结果图S1 中经过连通域搜寻算法后得到四个连通域初始结果
*C* = { *c1, c2, c3, c4* }，
这四个结果既代表了四个分割结果的中心区域，然后通过逐渐扩展kernel的区域来合并上*S2,
S3* 中的像素，最终获得图中最右侧的结果图。尺度扩展算法的描述如下：

![](/static/blog/media/a18618b87b7e4057bdb1b1fbfce8b0f5.png)

#### 真值(Ground Truth, GT)标记生成

这个框架的网络结构部分PSENet
能产生多个分割结果，那么相反的，在训练时，我们就得为训练集的每个kernel都分别标注一个真值，一个一个标注太麻烦了，通过先标注最大的kernel中的真值，再通过缩小文本行标注的大小来获取所有掩膜标注。本文所使用的是Vatti
多边形裁剪算法，将原本的多边形框*pn*缩小*di*个像素获得*pi*，然后将*pi*转化为二值的掩膜标注*G1,
G2, … , Gn*，*那么缩小di个像素究竟是缩小多少呢？*令缩放率为*ri ,*
Area为*pn*面积， Perimeter为*pn*周长，m为最小的尺度比例，得出的算法如下：

![](/static/blog/media/b53dfdde1ed3a99cbcca26846c9a5739.png)

![](/static/blog/media/cb03e3c2e240330bf14931875108ad8e.png)

#### 模型训练

PSENet的损失函数是：

![](/static/blog/media/11dc8fb087e098b994069a9a5e01f61f.png)

其中*Lc*代表了最终原图尺寸的文本行像素级别的分类结果的损失函数，*Ls*代表了其他的每一个缩小的kernel的分类结果的损失函数，λ用于平衡两种损失函数的比重。

然后是*Lc*和*Ls*的计算方法，由于使用二值的交叉熵容易使面积极小的文本行被网络误认为非文本行区域，本文使用的是分割任务中常用的**dice
coefficient**。这个损失函数的表达如下:

![](/static/blog/media/5b1ad556ed06a89fcf5f36fc34f45c0c.png)

其中*Sixy*和*Gixy*代表了在*Si*和*Gi*中位于*(x, y)*的像素值。

![](/static/blog/media/fe69ccb9410aece3ad9b4deb3c34af77.png)

*Lc*专注于分割文本行区域和非文本行区域，故使用**Online Hard Example
Mining(OHEM)**的训练掩膜图M。OHEM是一种基于F-RCN算法改进的高校目标检测算法。

![](/static/blog/media/6ccbe0b0e33cf691072caf36cbb98643.png)

*Ls*是缩放后的kernel的结果的损失函数，既然这些结果总是被最初的文本行的分割结果所包围，我们可以忽略分割结果中非文本行区域来避免冗余。*W*就是用于去除非文本行区域影响的掩膜图

### 实验结果 

#### 数据集

本文使用了CTW1500, Total-Text, ICDAR2015 和 ICDAR2017
MLT四个常用公共数据集作为我们的测试基准的验证集与其他的state-of-art
方法进行比较。用在ImageNet预训练过的ResNet作为我们模型的backbone，使用SGD优化器对所有的网络结构进行优化。模型训练时使用7200个
IC17-MLT 训练数据 和1800个IC17-MLT
验证数据混合起来作为训练集，batch-size为6，迭代次数为180K，使用4个GPU进行训练，初始学习率定为1x10e-3，并且在60K与120K的时候除以10。

#### 消融实验

![](/static/blog/media/1876af483271cd59ee54fd2d3f7246d3.png)

![](/static/blog/media/63e681a3f0c3895e9f868dd333ee95d8.png)

本文作者使用消融实验来判断**最小的kernel尺度**，**kernel输出**，**backbone的不同**会对模型表现造成的影响。

#### 量性结果比较

##### 弯曲的文本行检测

![](/static/blog/media/e162f26941cbdbd9399102eeae911b4d.png)

![](/static/blog/media/a413ee2216b86a13d0c6cf9c2a720202.png)

##### 不同方向的文本行检测

![](/static/blog/media/e2babe182767190d32636aee1b37caec.png)

##### 多语种的文本行检测

![](/static/blog/media/3fb64b2dcc00ba9ec4aad970eaf0316c.png)

![](/static/blog/media/f5a8a351d412ec5deb9d33e24927fbc5.png)

##### 运行速度分析

![](/static/blog/media/d0c29295287d2817b94aaab906e6ea0e.png)

#### 质性结果比较

![](/static/blog/media/4eb83882f96a8c3658662773d22c0f40.png)

![](/static/blog/media/bb8a0fecdc1f7254095f6e00eee78b66.png)

![](/static/blog/media/ff6cd20ecbbffce192531fa6c0d32dec.png)

![](/static/blog/media/d5adf3c1529f0be150f5931d29eb56d1.png)

![](/static/blog/media/9405a3d339e520d71614cd164956c0d5.png)
