# 笔记 

## 介绍
----

### 本文针对什么领域

纸质背景手写英文字序列识别，英文字母间含有大量粘连。

### 本文主要贡献

提出了一个不需要依赖任何上下文信息的识别文字的方法。

其主要贡献是：

1.  使用普通的CNN结构进行英文单词，单词块中多符号识别预测

2.  提出一种基于概率的字符错误率来从字符概率序列中计算单词概率

3.  创建从最近发布的NIST单个字符数据集导出的基于真实的基于块的数据集

4.  提出一种全卷积网络，在基于词典或不限制词典的手写体基准测试上展示最优秀的结果

本文提出的模型
--------------

### 方法结构

本文提出的方法的分为四个步骤：

1.  使用一个**给定词典的CNN**来检测常用单词(The, Her, and)

2.  如果没有找到常用单词，使用**块长CNN**预测一个图块中的符号数目并将图块缩放为规定的长度与宽度

3.  使用一种符号**预测FCN**进行符号预测

4.  如果单词图块中的单词属于已知的词汇表，使用**词汇表匹配**

本文中使用的所有的CNN的部分都是使用的共同的架构，VGG的变体：

**C(64)- C(64)- C(64)- P(2)- C(128)- C(128)- P(2)- C(256)- C(256)- P(2)- C(512)-
C(512)**

C(d, (h, w), pad(h, w))中d代表过滤器（卷积核）的数目，过滤器的大小h,
w默认3\*3，步长pad(h,
w)默认为1，填充默认为1。每一层后都会经过ReLU激活函数和BatchNormalization。但是各个网络的后半部分有所差异。

![](/static/blog/media/ba7652a2224c2eebb25f56813454e3f2.png)

#### 符号校准（Symbol alignment）

通常会使用CTC方法进行校准，但是模型很难收敛，准确度较差(\>50%
CER)。单词图块的宽度不固定，导致单词中间的空格的分布不统一。另外由于有些数据集中的样本太小导致得出的结果会比标签标注的要少。以上三种问题都在Jaderberg等人的论文中得到启发从而被解决。解决方案就是将输入的单词图块进行缩放，缩放到32\*128的大小后传入到块长CNN来对符号数目N进行估计预测。根据预测得到的单词数目N将样本在此缩放至32\*16N,再将第二次缩放后的图传入符号预测FCN中。这样能够使每个样本都能得到(2N+1)个预测值，即N个单词和N+1个空格

![](/static/blog/media/cde5932fcc56d73f328b5192b0f42d79.png)

#### 块长CNN（Block CNN）

FC(256)-FC(64)-Dropout(0.5)-FC(32),
最后会输出一个长度为32的one-hot值代表预测结果的单词长度，限制长度从1到32（至少有一个blank）

#### 符号预测FCN(Symbol Prediction FCN)

C(1024, (4, 4), (1, 2))-C(1024, (3, 9), (0, 4))-C(111, (1, 1), (0,
0))，由于单词图块在输入FCN前缩放至32\*16N的大小，经过前半段的CNN部分后大小会变为4\*2N，经过第两层全连接层后得到1\*2N+1\*111的结果向量。111代表110种符号与一个BLANK空格标识符。

![](/static/blog/media/396be0fb7bd5e19a7afd792909a63a51.png)

![](/static/blog/media/c8e88d3e4c81b5356f336f5463ecf4c9.png)

#### CER与词汇库匹配

本文使用一种归一化的字符错误率来显示最终结果的好坏，这一种错误率用公式来表达为：

```latex
CER = \frac{R + D + I}{R + D + I + C}
```

其中，R代表错误的需要被替换的字符数，D表示多余的需要被删除的字符数，I表示缺少的需要插入的字符数，C代表正确无误的字符数。为了去比较单词预测结果和真值的比较，我们使用动态规划算法来计算单词的CER。令h为预测长度，l为标签长度，我们的目标是计算$$CER = C_{h,l}$$，令$$p_i$$是第i个字符的预测值，$$L_j$$是第j个字符的标签值，列出递归式：


```latex
C_{0,0} = 0,
C_{i,j} = min(C_{i - 1,j},C_{i,j - 1},Diag)
```

```latex
Diag = 
C_{i - 1,j - 1}, if\ p_{i} = L_{j} 
C_{i - 1,j - 1} + 1, else 
```


为了提升应用场景中的表现，作者应用了一种基于错误率的词汇表匹配法。定义V是词汇表集合并且W(p)是基于序列p的单词预测。

```latex
W\left( p \right) = \text{argmin}_{L \in V}CER(p,L)
```

通过将得到最小的CER值来获得正确的单词预测

另外，通过加入单词出现的频繁度C(L)来进一步提升词汇表匹配的结果。

```latex
W\left( p \right) = \text{argmin}_{L \in V}\text{CER}\left( p,L \right) + \frac{1}{1 + C(L)}
```

#### 基于词典的预测

FC(2048)-Dropout(0.5)-FC(V), V代表词典的大小

#### 训练参数细节

本模型的学习率设置为0.01,并且每进行25000次迭代后就降低为原先的0.1倍，设置每一个迭代的批次大小为64，并且使用了L2归一化，$$\lambda
= 0.0025$$

本文提出的方法的效果如何
------------------------

### 数据集

本文所使用的训练数据集有IAM,RIMES,NIST的混合数据集，每一轮迭代中的数据会包含24个IAM样本，24个RIMES样本，16个NIST样本。但是使用的测试数据集也是上述数据集的并集。

![](/static/blog/media/10ab1be450f1cc5f3a3596def1f47d16.png)

### 实验效果图

![](/static/blog/media/34219ab773edf167eac4b428cb8969ca.png)

### 实验数据结果

#### 准确率比较

![](/static/blog/media/18995c79a59b8383ea21256ddd1e871c.png)

![](/static/blog/media/bba5f44746e94286e495626e4f183e54.png)
